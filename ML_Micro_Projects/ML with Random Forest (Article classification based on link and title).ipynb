{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/MSG/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/MSG/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#!/bin/env python3.6\n",
    "\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import os, re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this work, the idea of detecting phony links is taken from an article on <br>\n",
    "**TowardDataScience.com** about **\"Phishing URL Detection with ML\"** <br>\n",
    "link: https://towardsdatascience.com/phishing-domain-detection-with-ml-5be9c99293e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "filename = 'Sarcasm_Headlines_Dataset.json'\n",
    "full_file_name = os.path.join(path, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the json file and making a dataframe (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd= pd.read_json(full_file_name, chunksize=10000, lines=True)\n",
    "df = pd.concat(dd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the links with urlparse - urlparse gives scheme, netloc, path, params and query parts of a link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['parsed_links'] = df.article_link.apply(lambda x: urlparse(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of parse resutls: <br>\n",
    " - ParseResult(scheme='http', netloc='www.cwi.nl:80', path='/%7Eguido/Python.html', params='', query='', fragment='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>parsed_links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "      <td>(https, www.huffingtonpost.com, /entry/versace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "      <td>(https, www.huffingtonpost.com, /entry/roseann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "      <td>(https, local.theonion.com, /mom-starting-to-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "      <td>(https, politics.theonion.com, /boehner-just-w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>(https, www.huffingtonpost.com, /entry/jk-rowl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \\\n",
       "0  former versace store clerk sues over secret 'b...             0   \n",
       "1  the 'roseanne' revival catches up to our thorn...             0   \n",
       "2  mom starting to fear son's web series closest ...             1   \n",
       "3  boehner just wants wife to listen, not come up...             1   \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0   \n",
       "\n",
       "                                        parsed_links  \n",
       "0  (https, www.huffingtonpost.com, /entry/versace...  \n",
       "1  (https, www.huffingtonpost.com, /entry/roseann...  \n",
       "2  (https, local.theonion.com, /mom-starting-to-f...  \n",
       "3  (https, politics.theonion.com, /boehner-just-w...  \n",
       "4  (https, www.huffingtonpost.com, /entry/jk-rowl...  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating new feature by extracting the netloc and length of the path of each link "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['netloc'] = df.parsed_links.apply(lambda x: x.netloc)\n",
    "df['path'] = df.parsed_links.apply(lambda x: len(x.path))\n",
    "# For now only work on the length of path and neloc name! \n",
    "# df['query'] = df.parsed_links.apply(lambda x: x.query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorizing length of the path of each link - 'short16','short53', 'medium65', 'long100', 'crazylong184' for  links with character length of 15, 53, 65, 67, 100 and 185. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [15, 53, 65, 67, 100, 185]\n",
    "category = pd.cut(df.path, bins, labels=['short16','short53', 'medium65', 'long100', 'crazylong184'])\n",
    "category = category.to_frame()\n",
    "category.columns = ['category_length']\n",
    "df = pd.concat([df, category], axis=1)\n",
    "df['text'] = df[['netloc', 'category_length', 'headline']].apply(lambda x: ' '.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to clean, stem and lemattize the text to make it ready for trainig our model with: \n",
    "In this psrt we do steps as below:\n",
    "    \n",
    "- Removing all the special characters\n",
    "- Removing all single characters\n",
    "- Removing single characters from the start\n",
    "- Substituting multiple spaces with single space\n",
    "- Removing prefixed 'b'\n",
    "- Converting to Lowercase\n",
    "- Lemmatization based on WordNet's morphy function\n",
    "- Stemming using the Porter Stemming Algorithm,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.866698980331421\n"
     ]
    }
   ],
   "source": [
    "def clean_stem_lemattize(x):\n",
    "    \"\"\"Applies some pre-processing on the given text.\n",
    "\n",
    "        Steps :\n",
    "        - Removing all the special characters\n",
    "        - Removing all single characters\n",
    "        - Removing single characters from the start\n",
    "        - Substituting multiple spaces with single space\n",
    "        - Removing prefixed 'b'\n",
    "        - Converting to Lowercase\n",
    "        - Lemmatization based on WordNet's morphy function\n",
    "        - Stemming using the Porter Stemming Algorithm,\n",
    "        \"\"\"\n",
    "    x = re.sub(r'\\W+', ' ', x)\n",
    "    x = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', x)\n",
    "    x = re.sub(r'\\^[a-zA-Z]\\s+', ' ', x)\n",
    "    x = re.sub(r'\\s+', ' ',x)\n",
    "    x = x.lower()\n",
    "    x = [porter_stemmer.stem(wordnet_lemmatizer.lemmatize(word)) for word in x.split()]\n",
    "    \n",
    "    return ' '.join(x)\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "df.text = df.text.apply(clean_stem_lemattize)\n",
    "print(time.time() -  start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now time to train our model. First, we need to vectorize our text and then split our data to train and test sections. <br>\n",
    "- I chose RandomForestClassifier as a good model for this classification problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Splitting Train_Test\n",
      "Starting Fitting Model\n",
      "##############################\n",
      "Starting Prediction!!\n",
      "##############################\n",
      "Starting test Score!!!\n",
      "##############################\n",
      "[[3007    0]\n",
      " [   0 2335]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3007\n",
      "           1       1.00      1.00      1.00      2335\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      5342\n",
      "   macro avg       1.00      1.00      1.00      5342\n",
      "weighted avg       1.00      1.00      1.00      5342\n",
      "\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "X = df.text\n",
    "y = df.is_sarcastic\n",
    "tfidfconverter = TfidfVectorizer(max_features=1500, min_df=5, max_df=0.65, stop_words=stopwords.words('english'))\n",
    "X = tfidfconverter.fit_transform(documents).toarray()\n",
    "\n",
    "\n",
    "print('Starting Splitting Train_Test')\n",
    "print('#'*30)\n",
    "\n",
    "# dividing data into 20% test set and 80% training set.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "print('Starting Fitting Model')\n",
    "print('#'*30)\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "print('Starting Prediction!!')\n",
    "print('#'*30)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    \n",
    "#%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%#\n",
    "#%!% -------------------This part is for checking accuracy of the model!--------------------- %!%!#\n",
    "#%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%!%#\n",
    "\n",
    "print('Starting test Score!!!')\n",
    "print('#'*30)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Done! Our model predicts whether an article is sarcastic (phony) or not based on its link and the headline of the article! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
